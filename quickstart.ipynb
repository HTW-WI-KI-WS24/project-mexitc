{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\guzma\\anaconda3\\lib\\site-packages (0.0.316)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (3.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (0.0.62)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: pytest-subtests<0.12.0,>=0.11.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from langsmith<0.1.0,>=0.0.43->langchain) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: pytest>=7.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.43->langchain) (7.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.43->langchain) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.43->langchain) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.43->langchain) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\guzma\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.clear()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9, openai_api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.9,openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy\n",
      "2. Naples, Italy\n",
      "3. Bologna, Italy\n",
      "4. Florence, Italy\n",
      "5. Venice, Italy\n"
     ]
    }
   ],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= ['food'],\n",
    "    template= \"What are 5 vacation destinations for someone who likes to eat {food}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are 5 vacation destinations for someone who likes to eat dessert?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(food=\"dessert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France – for its world-famous pastries and chocolate shops\n",
      "2. Rome, Italy – for its wide variety of gelato flavors\n",
      "3. Tokyo, Japan – for its unique mochi treats\n",
      "4. New York City, USA – for its iconic cheesecakes and cupcakes\n",
      "5. Honolulu, Hawaii – for its delicious shave ice with sweet toppings\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=\"dessert\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains: Combine llms and promts in multi step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll be using the same prompt tamplate and llm defined earlier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining the llm and prompt template allows us to be faster, instead of feeding the llm with the promp template by ourserlves the llm chain does it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Puerto Rico - Home to a rich variety of tropical fruits such as coconuts, mangos, guavas, and papayas.\n",
      "\n",
      "2. Hawaii - The perfect spot for tropical paradise with a variety of exotic fruits like lychee, star fruit, and passion fruit.\n",
      "\n",
      "3. Florida - An abundance of citrus fruits such as oranges, grapefruits, and tangerines.\n",
      "\n",
      "4. California - Enjoy an array of seasonal fruits like apples, peaches, and pears.\n",
      "\n",
      "5. Mexico - Sample a variety of sweet and juicy tropical fruits such as bananas, papayas, and pineapples.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run('fruit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be connecting openAI with google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll be using the same llm model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools,llm,agent=\"zero-shot-react-description\",verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should first find out the age of the current leader of Japan\n",
      "Action: Search\n",
      "Action Input: \"Current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida is the current prime minister of Japan, replacing Yoshihide Suga on 4 October 2021. As of 18 November 2023, there have been 64 individual prime ministers serving 101 terms of office.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The current age of Prime Minister Kishida would be the most relevant \n",
      "Action: Search\n",
      "Action Input: \"Age of Fumio Kishida\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m66 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use a calculator to find the largest prime number smaller than the age of Prime Minister Kishida\n",
      "Action: Calculator\n",
      "Action Input: 66\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 66\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The largest prime number that is smaller than the age of the current leader of Japan is 66.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The largest prime number that is smaller than the age of the current leader of Japan is 66.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the current leader of japan? What is the largest prime number that is smaller than their age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "convo = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input= \"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm just having a conversation with an AI\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's nice to meet you. I'm an AI that specializes in providing information about a variety of topics. What would you like to know?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"I'm just having a conversation with an AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm just having a conversation with an AI\n",
      "AI:  That's great! It's nice to meet you. I'm an AI that specializes in providing information about a variety of topics. What would you like to know?\n",
      "Human: What was the first thing I told you? give me an alternative for what i said\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You told me \"Hi there\". An alternative phrase you could have used is \"Hello there\".'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.predict(input=\"What was the first thing I told you? give me an alternative for what i said\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
